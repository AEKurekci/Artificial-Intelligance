# -*- coding: utf-8 -*-
"""CENG3526_IR_HW_AliEmreKurekci.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DQIxBCjJC4_brUQYnSanl8Fn76LCFaYn

### Read from zip file

To use this code, upload the appropirate *.zip file to colab.

Example code expects the "30Columnists.zip" file be uploded and ready in the GCE backend. 

To do that, you can also mount your **google drive** and use the datasets from there.

Also you can use the **git repo** to download the zip files: 

https://github.com/dtaner/CENG3526/blob/master/30Columnists.zip

or check out (clone) the repo to use datasets.

```
# Clone the entire repo.
!git clone -l -s https://github.com/dtaner/CENG3526.git cloned-repo
%cd cloned-repo
!ls
```

# The first assignment in this homework.
Play with the code and do your own excercise to get familiar with reading texts from files.
"""

import io, os
import re as re
import zipfile as zipfile
import copy
import math
import time
import json

def checkWord(word):
    chars = ["-", "", "*", "_", "—", "–", "&", "=", "%", "…"]
    if word in chars: return False
    else: return True
def isNotStopWord(word):
    stopWords = ["of", "a", "is", "are", "to" , "he", "she", "it", "an",
                 "and","as","at","be","by","for","from","has",
                 "in","its","on","that","the","to","was","were",
                 "will","with","so","but","or","his","her","him","them",
                 "am","their","this","us"]#we you i they
    if word in stopWords: return False
    else: return True

def findInvertedIndex(isNumberTokenize):
    docCount = 0
    lineArray = []
    allWordArray = []
    wordCounts = {}
    with zipfile.ZipFile('30Columnists.zip') as z:
        for zipinfo in z.infolist():
            if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):
                with z.open(zipinfo) as f:
                    textfile = io.TextIOWrapper(f, encoding='cp1254', newline='')
                    docCount += 1
                    wordCounts[textfile.name] = 0
                    for line in textfile:
                        lineArray = line.strip().lower().split()
                        for ind,word in enumerate(lineArray):
                            #clening point marks
                            if len(re.findall("^\xad+", word)) > 0:
                                word = word[1:]
                                lineArray[ind] = word
                            if len(re.findall("\xad+$", word)) > 0:
                                word = word[:-1]
                                lineArray[ind] = word
                            while(len(re.findall("^[',;.:½¿•<>…/\\\\“‘”’\+\{\}\*\"\[\(\-]+", word)) > 0):
                                word = word[1:]
                                lineArray[ind] = word
                            while(len(re.findall("['.,?!:;½¿•<>…/\\\\“‘”’\+\{\}\*\"\)\]]+$", word)) > 0):
                                word = word[:-1]
                                lineArray[ind] = word
                            if checkWord(word) and isNotStopWord(word):
                                if isNumberTokenize:
                                    if len(re.findall("[0-9]+", word)) <= 0:
                                        wordArray = [word, textfile.name]
                                        allWordArray.append(wordArray)
                                else:
                                    wordArray = [word, textfile.name]
                                    allWordArray.append(wordArray)
                                wordCounts[textfile.name] += 1
              
    return allWordArray, docCount, wordCounts

startTime = time.time()
invertedIndex, totalDoc, wordCountDict = findInvertedIndex(True)
finishTime = time.time()
print("Inverted Index Run Time(sn): ",finishTime-startTime)
print("Total Document Number: ", totalDoc)
print("Total Word Numbers Of Documents: ", wordCountDict)

startTime = time.time()
invertedIndex.sort()
finishTime = time.time()
print("Sorting Run Time: ",finishTime-startTime)

def mergeWords(arr):
    counter = 0
    poppedIndexes = []
    posList = []
    allPostingLists = {}
    mergedArr = []
    tfOfWord = {}
    allTf = {}
    comparingWord = arr[0][0]
    comparingDoc = arr[0][1]
    for i, j in enumerate(arr):
        if comparingWord == j[0] and comparingDoc == j[1]:                
            counter += 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        elif comparingWord == j[0] and comparingDoc != j[1]:
            posList.append(comparingDoc)
            comparingDoc = j[1]
            counter = 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        else:
            mergedArr.append(comparingWord)
            posList.append(comparingDoc)
            allPostingLists[comparingWord] = copy.deepcopy(posList)
            posList = []
            counter = 1
            comparingWord = j[0]
            comparingDoc = j[1]
            tfOfWord = {}
            tfOfWord[comparingDoc] = 1
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
    return mergedArr,allTf,allPostingLists

startTime = time.time()
mergedInvertedIndex, termFrequenceDict, postingList = mergeWords(invertedIndex)
finishTime = time.time()
print("Merging Inverted Index Run Time: ",finishTime-startTime)

def findTFIDF(relevantDocs, wordDict, idf):
    allTFIDFScores = {}
    for docId, val in relevantDocs.items():
        allTFIDFScores[docId] = float(val / wordDict[docId]) * idf
    
    return allTFIDFScores

def search(text, theIndexes, word_counts):
    if  not isNotStopWord(text):
        print("No result")
        return
    textArr = text.lower().split()
    wordIDF = math.log10(totalDoc/len(postingList[textArr[0]]))
    TFIDF = findTFIDF(theIndexes[textArr[0]], word_counts, wordIDF)
    textArr.pop(0)
    for word in textArr:
        if word in theIndexes:
            tfidf = findTFIDF(theIndexes[word], word_counts, wordIDF)
            for i, val in tfidf.items():
                if i not in TFIDF:
                    TFIDF[i] = val 
                else:
                    TFIDF[i] += val
    TFIDF = dict(sorted(TFIDF.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    for page, score in TFIDF.items():
        print(page+" ", score)
    print(len(TFIDF.keys()), " results are found")

Query1 = "Hello World"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query1, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query2 = "Cristiano Ronaldo Mourinho"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query2, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query3 = "cobra obama insurance health"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query3, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query4 = "cobra"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query4, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query5 = "Student of Computer Engineering"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query5, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query6 = "I don't know to use solr"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query6, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query7 = "Some columnists are relevant football"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query7, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query8 = "Football"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query8, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query9 = "Finance"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query9, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query10 = "Financial crisis"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query10, termFrequenceDict, wordCountDict)
finishTime = time.time()
print("search time: ", finishTime - startTime)

with open("data.json", "w") as outfile:
    json.dump(termFrequenceDict, outfile)