# -*- coding: utf-8 -*-
"""CENG3526_IR_HW_AliEmreKurekci.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DQIxBCjJC4_brUQYnSanl8Fn76LCFaYn

### Read from zip file

To use this code, upload the appropirate *.zip file to colab.

Example code expects the "30Columnists.zip" file be uploded and ready in the GCE backend. 

To do that, you can also mount your **google drive** and use the datasets from there.

Also you can use the **git repo** to download the zip files: 

https://github.com/dtaner/CENG3526/blob/master/30Columnists.zip

or check out (clone) the repo to use datasets.

```
# Clone the entire repo.
!git clone -l -s https://github.com/dtaner/CENG3526.git cloned-repo
%cd cloned-repo
!ls
```

# The first assignment in this homework.
Play with the code and do your own excercise to get familiar with reading texts from files.
"""

import io, os
import re as re
import zipfile as zipfile
import copy
import math
import time
import json

K1 = 1.6
B = 0.75
listPageSize = 5

def checkWord(word):
    chars = ["-", "", "*", "_", "—", "–", "&", "=", "%", "…"]
    if word in chars: return False
    else: return True


def isStopWord(word):
    stopWords = ['of', 'a', 'is', 'are', 'to', 'he', 'she', 'it', 'an', 'and',
                 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'in', 'its',
                 'on', 'that', 'the', 'was', 'were', 'will', 'with',
                 'so', 'but', 'or', 'his', 'her', 'him', 'them', 'am', 'their',
                 'this', 'us', 'ourselves', 'hers', 'between', 'yourself',
                 'again', 'there', 'about', 'once', 'during', 'out', 'very',
                 'having', 'they', 'own', 'some', 'do', 'yours', 'such', 'into',
                 'most', 'itself', 'other', 'off', 's', 'who', 'each',
                 'themselves', 'until', 'below', 'we', 'these', 'your',
                 'through', 'don', 'nor', 'me', 'more', 'himself', 'down',
                 'should', 'our', 'while', 'above', 'both', 'up', 'ours',
                 'had', 'all', 'no', 'when', 'any', 'before', 'same', 'been',
                 'have', 'does', 'yourselves', 'then', 'because', 'what',
                 'over', 'why', 'can', 'did', 'not', 'now', 'under', 'you',
                 'herself', 'just', 'where', 'too', 'only', 'myself', 'which',
                 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if',
                 'theirs', 'my', 'against', 'doing', 'how', 'further', 'here',
                 'than']
    stopWords2 = ["0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2",
                 "a3", "a4", "ab", "able", "about", "above", "abst", "ac",
                 "accordance", "according", "accordingly", "across", "act",
                 "actually", "ad", "added", "adj", "ae", "af", "affected", 
                 "affecting", "affects", "after", "afterwards", "ag", "again",
                 "against", "ah", "ain", "ain't", "aj", "al", "all", "allow",
                 "allows", "almost", "alone", "along", "already", "also",
                 "although", "always", "am", "among", "amongst", "amoungst",
                 "amount", "an", "and", "announce", "another", "any", "anybody",
                 "anyhow", "anymore", "anyone", "anything", "anyway", "anyways",
                 "anywhere", "ao", "ap", "apart", "apparently", "appear",
                  "appreciate", "appropriate", "approximately", "ar", "are", 
                  "aren", "arent", "aren't", "arise", "around", "as", "a's",
                  "aside", "ask", "asking", "associated", "at", "au", "auth",
                  "av", "available", "aw", "away", "awfully", "ax", "ay", "az",
                  "b", "b1", "b2", "b3", "ba", "back", "bc", "bd", "be", 
                  "became", "because", "become", "becomes", "becoming",
                  "been", "before", "beforehand", "begin", "beginning",
                  "beginnings", "begins", "behind", "being", "believe",
                  "below", "beside", "besides", "best", "better", "between",
                  "beyond", "bi", "bill", "biol", "bj", "bk", "bl", "bn",
                  "both", "bottom", "bp", "br", "brief", "briefly", "bs", "bt",
                  "bu", "but", "bx", "by", "c", "c1", "c2", "c3", "ca", "call", "came", "can", "cannot", "cant", "can't", "cause", "causes", "cc", "cd", "ce", "certain", "certainly", "cf", "cg", "ch", "changes", "ci", "cit", "cj", "cl", "clearly", "cm", "c'mon", "cn", "co", "com", "come", "comes", "con", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn", "couldnt", "couldn't", "course", "cp", "cq", "cr", "cry", "cs", "c's", "ct", "cu", "currently", "cv", "cx", "cy", "cz", "d", "d2", "da", "date", "dc", "dd", "de", "definitely", "describe", "described", "despite", "detail", "df", "di", "did", "didn", "didn't", "different", "dj", "dk", "dl", "do", "does", "doesn", "doesn't", "doing", "don", "done", "don't", "down", "downwards", "dp", "dr", "ds", "dt", "du", "due", "during", "dx", "dy", "e", "e2", "e3", "ea", "each", "ec", "ed", "edu", "ee", "ef", "effect", "eg", "ei", "eight", "eighty", "either", "ej", "el", "eleven", "else", "elsewhere", "em", "empty", "en", "end", "ending", "enough", "entirely", "eo", "ep", "eq", "er", "es", "especially", "est", "et", "et-al", "etc", "eu", "ev", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere", "ex", "exactly", "example", "except", "ey", "f", "f2", "fa", "far", "fc", "few", "ff", "fi", "fifteen", "fifth", "fify", "fill", "find", "fire", "first", "five", "fix", "fj", "fl", "fn", "fo", "followed", "following", "follows", "for", "former", "formerly", "forth", "forty", "found", "four", "fr", "from", "front", "fs", "ft", "fu", "full", "further", "furthermore", "fy", "g", "ga", "gave", "ge", "get", "gets", "getting", "gi", "give", "given", "gives", "giving", "gj", "gl", "go", "goes", "going", "gone", "got", "gotten", "gr", "greetings", "gs", "gy", "h", "h2", "h3", "had", "hadn", "hadn't", "happens", "hardly", "has", "hasn", "hasnt", "hasn't", "have", "haven", "haven't", "having", "he", "hed", "he'd", "he'll", "hello", "help", "hence", "her", "here", "hereafter", "hereby", "herein", "heres", "here's", "hereupon", "hers", "herself", "hes", "he's", "hh", "hi", "hid", "him", "himself", "his", "hither", "hj", "ho", "home", "hopefully", "how", "howbeit", "however", "how's", "hr", "hs", "http", "hu", "hundred", "hy", "i", "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ignored", "ih", "ii", "ij", "il", "i'll", "im", "i'm", "immediate", "immediately", "importance", "important", "in", "inasmuch", "inc", "indeed", "index", "indicate", "indicated", "indicates", "information", "inner", "insofar", "instead", "interest", "into", "invention", "inward", "io", "ip", "iq", "ir", "is", "isn", "isn't", "it", "itd", "it'd", "it'll", "its", "it's", "itself", "iv", "i've", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "just", "k", "ke", "keep", "keeps", "kept", "kg", "kj", "km", "know", "known", "knows", "ko", "l", "l2", "la", "largely", "last", "lately", "later", "latter", "latterly", "lb", "lc", "le", "least", "les", "less", "lest", "let", "lets", "let's", "lf", "like", "liked", "likely", "line", "little", "lj", "ll", "ll", "ln", "lo", "look", "looking", "looks", "los", "lr", "ls", "lt", "ltd", "m", "m2", "ma", "made", "mainly", "make", "makes", "many", "may", "maybe", "me", "mean", "means", "meantime", "meanwhile", "merely", "mg", "might", "mightn", "mightn't", "mill", "million", "mine", "miss", "ml", "mn", "mo", "more", "moreover", "most", "mostly", "move", "mr", "mrs", "ms", "mt", "mu", "much", "mug", "must", "mustn", "mustn't", "my", "myself", "n", "n2", "na", "name", "namely", "nay", "nc", "nd", "ne", "near", "nearly", "necessarily", "necessary", "need", "needn", "needn't", "needs", "neither", "never", "nevertheless", "new", "next", "ng", "ni", "nine", "ninety", "nj", "nl", "nn", "no", "nobody", "non", "none", "nonetheless", "noone", "nor", "normally", "nos", "not", "noted", "nothing", "novel", "now", "nowhere", "nr", "ns", "nt", "ny", "o", "oa", "ob", "obtain", "obtained", "obviously", "oc", "od", "of", "off", "often", "og", "oh", "oi", "oj", "ok", "okay", "ol", "old", "om", "omitted", "on", "once", "one", "ones", "only", "onto", "oo", "op", "oq", "or", "ord", "os", "ot", "other", "others", "otherwise", "ou", "ought", "our", "ours", "ourselves", "out", "outside", "over", "overall", "ow", "owing", "own", "ox", "oz", "p", "p1", "p2", "p3", "page", "pagecount", "pages", "par", "part", "particular", "particularly", "pas", "past", "pc", "pd", "pe", "per", "perhaps", "pf", "ph", "pi", "pj", "pk", "pl", "placed", "please", "plus", "pm", "pn", "po", "poorly", "possible", "possibly", "potentially", "pp", "pq", "pr", "predominantly", "present", "presumably", "previously", "primarily", "probably", "promptly", "proud", "provides", "ps", "pt", "pu", "put", "py", "q", "qj", "qu", "que", "quickly", "quite", "qv", "r", "r2", "ra", "ran", "rather", "rc", "rd", "re", "readily", "really", "reasonably", "recent", "recently", "ref", "refs", "regarding", "regardless", "regards", "related", "relatively", "research", "research-articl", "respectively", "resulted", "resulting", "results", "rf", "rh", "ri", "right", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "said", "same", "saw", "say", "saying", "says", "sc", "sd", "se", "sec", "second", "secondly", "section", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "sf", "shall", "shan", "shan't", "she", "shed", "she'd", "she'll", "shes", "she's", "should", "shouldn", "shouldn't", "should've", "show", "showed", "shown", "showns", "shows", "si", "side", "significant", "significantly", "similar", "similarly", "since", "sincere", "six", "sixty", "sj", "sl", "slightly", "sm", "sn", "so", "some", "somebody", "somehow", "someone", "somethan", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "sp", "specifically", "specified", "specify", "specifying", "sq", "sr", "ss", "st", "still", "stop", "strongly", "sub", "substantially", "successfully", "such", "sufficiently", "suggest", "sup", "sure", "sy", "system", "sz", "t", "t1", "t2", "t3", "take", "taken", "taking", "tb", "tc", "td", "te", "tell", "ten", "tends", "tf", "th", "than", "thank", "thanks", "thanx", "that", "that'll", "thats", "that's", "that've", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "thered", "therefore", "therein", "there'll", "thereof", "therere", "theres", "there's", "thereto", "thereupon", "there've", "these", "they", "theyd", "they'd", "they'll", "theyre", "they're", "they've", "thickv", "thin", "think", "third", "this", "thorough", "thoroughly", "those", "thou", "though", "thoughh", "thousand", "three", "throug", "through", "throughout", "thru", "thus", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "together", "too", "took", "top", "toward", "towards", "tp", "tq", "tr", "tried", "tries", "truly", "try", "trying", "ts", "t's", "tt", "tv", "twelve", "twenty", "twice", "two", "tx", "u", "u201d", "ue", "ui", "uj", "uk", "um", "un", "under", "unfortunately", "unless", "unlike", "unlikely", "until", "unto", "uo", "up", "upon", "ups", "ur", "us", "use", "used", "useful", "usefully", "usefulness", "uses", "using", "usually", "ut", "v", "va", "value", "various", "vd", "ve", "ve", "very", "via", "viz", "vj", "vo", "vol", "vols", "volumtype", "vq", "vs", "vt", "vu", "w", "wa", "want", "wants", "was", "wasn", "wasnt", "wasn't", "way", "we", "wed", "we'd", "welcome", "well", "we'll", "well-b", "went", "were", "we're", "weren", "werent", "weren't", "we've", "what", "whatever", "what'll", "whats", "what's", "when", "whence", "whenever", "when's", "where", "whereafter", "whereas", "whereby", "wherein", "wheres", "where's", "whereupon", "wherever", "whether", "which", "while", "whim", "whither", "who", "whod", "whoever", "whole", "who'll", "whom", "whomever", "whos", "who's", "whose", "why", "why's", "wi", "widely", "will", "willing", "wish", "with", "within", "without", "wo", "won", "wonder", "wont", "won't", "words", "world", "would", "wouldn", "wouldnt", "wouldn't", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx", "y", "y2", "yes", "yet", "yj", "yl", "you", "youd", "you'd", "you'll", "your", "youre", "you're", "yours", "yourself", "yourselves", "you've", "yr", "ys", "yt", "z", "zero", "zi", "zz"]
    if word in stopWords2: return True
    else: return False


def findInvertedIndex(isNumberTokenize=True):
    docCount = 0
    lineArray = []
    allWordArray = []
    wordCounts = {}
    with zipfile.ZipFile('30Columnists.zip') as z:
        for zipinfo in z.infolist():
            if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):
                with z.open(zipinfo) as f:
                    textfile = io.TextIOWrapper(f, encoding='cp1254', newline='')
                    docCount += 1
                    file_name = str(zipinfo.filename)
                    file_name = file_name.split("/")
                    file_name = file_name[-1]
                    wordCounts[file_name] = 0
                    for line in textfile:
                        lineArray = line.strip().lower().split()
                        for ind,word in enumerate(lineArray):
                            #clening point marks
                            if len(re.findall("^\xad+", word)) > 0:
                                word = word[1:]
                                lineArray[ind] = word
                            if len(re.findall("\xad+$", word)) > 0:
                                word = word[:-1]
                                lineArray[ind] = word
                            while(len(re.findall("^[',;.:½¿•<>…/\\\\“‘”’\+\{\}\*\"\[\(\-]+", word)) > 0):
                                word = word[1:]
                                lineArray[ind] = word
                            while(len(re.findall("['.,?!:;½¿•<>…/\\\\“‘”’\+\{\}\*\"\)\]]+$", word)) > 0):
                                word = word[:-1]
                                lineArray[ind] = word
                            if checkWord(word) and not isStopWord(word):
                                if isNumberTokenize:
                                    if len(re.findall("[0-9]+", word)) <= 0:
                                        wordArray = [word, file_name]
                                        allWordArray.append(wordArray)
                                else:
                                    wordArray = [word, file_name]
                                    allWordArray.append(wordArray)
                                wordCounts[file_name] += 1
              
    return allWordArray, docCount, wordCounts


startTime = time.time()
invertedIndex, totalDoc, wordCountDict = findInvertedIndex()
finishTime = time.time()
print("Inverted Index Run Time(sn): ",finishTime-startTime)
print("Total Document Number: ", totalDoc)
print("Total Word Numbers Of Documents: ", wordCountDict)

startTime = time.time()
invertedIndex.sort()
finishTime = time.time()
print("Sorting Run Time: ",finishTime-startTime)

def mergeWords(arr):
    counter = 0
    poppedIndexes = []
    posList = []
    allPostingLists = {}
    mergedArr = []
    tfOfWord = {}
    allTf = {}
    comparingWord = arr[0][0]
    comparingDoc = arr[0][1]
    for i, j in enumerate(arr):
        if comparingWord == j[0] and comparingDoc == j[1]:                
            counter += 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        elif comparingWord == j[0] and comparingDoc != j[1]:
            posList.append(comparingDoc)
            comparingDoc = j[1]
            counter = 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        else:
            mergedArr.append(comparingWord)
            posList.append(comparingDoc)
            allPostingLists[comparingWord] = copy.deepcopy(posList)
            posList = []
            counter = 1
            comparingWord = j[0]
            comparingDoc = j[1]
            tfOfWord = {}
            tfOfWord[comparingDoc] = 1
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
    return mergedArr,allTf,allPostingLists

startTime = time.time()
mergedInvertedIndex, term_frequency_dict, postingList = mergeWords(invertedIndex)
finishTime = time.time()
print("Merging Inverted Index Run Time: ",finishTime-startTime)

def findAVGDL():
    lenghts = list(wordCountDict.values())
    sumOfLengths = 0
    for val in lenghts:
        sumOfLengths += val
    return sumOfLengths/totalDoc


def findIDF(word):
    return math.log10(totalDoc/len(postingList[word]))


def findBM25(relevant_docs, word):
    allBM25Scores = {}
    for docId, val in relevant_docs.items():
        dl = float(wordCountDict[docId])
        avgdl = float(findAVGDL())
        allBM25Scores[docId] = findIDF(word) * ((val * (K1 + 1)) /
                                                (val + (K1 *((1 - B) + (B * 
                                                            (dl / avgdl ))))))
    return allBM25Scores


def listForBM25(words_in_query):
    BM25 = findBM25(term_frequency_dict[words_in_query[0]], words_in_query[0])
    words_in_query.pop(0)
    for word in words_in_query:
        if word in term_frequency_dict:
            bm25 = findBM25(term_frequency_dict[word], word)
            for doc, val in bm25.items():
                if doc not in BM25:
                    BM25[doc] = val 
                else:
                    BM25[doc] += val
    BM25 = dict(sorted(BM25.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return BM25


def findTFIDF(relevantDocs, word):
    allTFIDFScores = {}
    for docId, val in relevantDocs.items():
        allTFIDFScores[docId] = float(val / wordCountDict[docId]) * findIDF(word)
    return allTFIDFScores


def listForTFIDF(wordsInQuery):
    TFIDF = findTFIDF(term_frequency_dict[wordsInQuery[0]], wordsInQuery[0])
    wordsInQuery.pop(0)
    for word in wordsInQuery:
        if word in term_frequency_dict:
            tfidf = findTFIDF(term_frequency_dict[word], word)
            for doc, val in tfidf.items():
                if doc not in TFIDF:
                    TFIDF[doc] = val 
                else:
                    TFIDF[doc] += val
    TFIDF = dict(sorted(TFIDF.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return TFIDF


def findE(TFi, doc, N):
    return (TFi * wordCountDict[doc]) / N


def findDFI(relevant_docs, word, N):
    allDIFScores = {}
    TFi = 0
    for tf in relevant_docs.values():
        TFi += tf
    for docId, tf in relevant_docs.items():
        allDIFScores[docId] = math.log10(((tf - findE(TFi, docId, N))/
                                         math.sqrt(findE(TFi, docId, N))) + 1)
    return allDIFScores


def listForDFI(words_in_query, N):
    DFI = findDFI(term_frequency_dict[words_in_query[0]], words_in_query[0], N)
    words_in_query.pop(0)
    for word in words_in_query:
        if word in term_frequency_dict:
            dfi = findDFI(term_frequency_dict[word], word, N)
            for doc, score in dfi.items():
                if doc not in DFI:
                    DFI[doc] = score 
                else:
                    DFI[doc] += score
    DFI = dict(sorted(DFI.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return DFI


def search(query, test_related):
    if isStopWord(query):
        print("No result")
        return
    textArr = query.lower().split()
    cleanText = []
    for ind,text in enumerate(textArr):
        if not isStopWord(text):
            cleanText.append(text)
    textArr.clear()
    textArr = cleanText
    for ind,word in enumerate(textArr):
        #clening point marks
        if len(re.findall("^\xad+", word)) > 0:
            word = word[1:]
            textArr[ind] = word
        if len(re.findall("\xad+$", word)) > 0:
            word = word[:-1]
            textArr[ind] = word
        while(len(re.findall("^[',;.:½¿•<>…/\\\\“‘”’\+\{\}\*\"\[\(\-]+", word)) > 0):
            word = word[1:]
            textArr[ind] = word
        while(len(re.findall("['.,?!:;½¿•<>…/\\\\“‘”’\+\{\}\*\"\)\]]+$", word)) > 0):
            word = word[:-1]
            textArr[ind] = word
    
    N = 0
    for val in wordCountDict.values():
        N += val
    resultsTFIDF = listForTFIDF(copy.deepcopy(textArr))
    resultsBM25 = listForBM25(copy.deepcopy(textArr))
    resultsDFI = listForDFI(copy.deepcopy(textArr), N)
    tfidfPrec = 0
    bm25Prec = 0
    dfiPrec = 0
    counter = 0
    print("===================TFIDF====================")
    for page, score in resultsTFIDF.items():
        counter += 1
        if page in test_related:
            tfidfPrec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    tfidfPrec = float(tfidfPrec / listPageSize)
    print(len(resultsTFIDF.keys()), " results are found with precision ", tfidfPrec)

    print("===================BM25====================")
    
    for page, score in resultsBM25.items():
        counter += 1
        if page in test_related:
            bm25Prec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    bm25Prec = float(bm25Prec / listPageSize)
    print(len(resultsBM25.keys()), " results are found with precision ", bm25Prec)

    print("==================DFI=====================")
    
    for page, score in resultsDFI.items():
        counter += 1
        if page in test_related:
            dfiPrec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    dfiPrec = float(dfiPrec / listPageSize)
    print(len(resultsDFI.keys()), " results are found with precision ", dfiPrec)


Query1 = "Great news! The war is over, we are free."
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query1, ["1.14.txt", "3.25.txt", "7.37.txt", "6.27.txt", "30.17.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query3 = "COBRA insurance health service"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query3, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query4 = "cobra"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query4, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query10 = "the biggest Financial crisis"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query10, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query7 = "high value of football economy"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query7, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query8 = "future job opportunity"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query8, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query9 = "increasing health cost"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query9, [])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query6 = "rich gold man a party last night"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query6, ["7.14.txt", "15.50.txt", "10.19.txt", "4.14.txt", "7.13.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query5 = "I want to go to a vacation."
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query5, ["25.20.txt",  "23.8.txt", "15.49.txt", "24.38.txt", "24.28.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query2 = 'The rich "gold" man love rich "pink" queen. I love this situation!"'
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query2, ["10.29.txt", "30.44.txt", "4.34.txt", "10.19.txt", "15.50.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

with open("data.json", "w") as outfile:
    json.dump(termFrequenceDict, outfile)