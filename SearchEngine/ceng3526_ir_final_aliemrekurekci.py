# -*- coding: utf-8 -*-
"""CENG3526_IR_Final_AliEmreKurekci.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DQIxBCjJC4_brUQYnSanl8Fn76LCFaYn

### Read from zip file

To use this code, upload the appropirate *.zip file to colab.

Example code expects the "30Columnists.zip" file be uploded and ready in the GCE backend. 

To do that, you can also mount your **google drive** and use the datasets from there.

Also you can use the **git repo** to download the zip files: 

https://github.com/dtaner/CENG3526/blob/master/30Columnists.zip

or check out (clone) the repo to use datasets.

```
# Clone the entire repo.
!git clone -l -s https://github.com/dtaner/CENG3526.git cloned-repo
%cd cloned-repo
!ls
```

# The first assignment in this homework.
Play with the code and do your own excercise to get familiar with reading texts from files.
"""

import io, os
import re as re
import zipfile as zipfile
import copy
import math
import time
import json

K1 = 1.6
B = 0.75
listPageSize = 5
total_tfidf_prec = []
total_bm25_prec = []
total_dfi_prec = []

def checkWord(word):
    chars = ["-", "", "*", "_", "—", "–", "&", "=", "%", "…"]
    if word in chars: return False
    else: return True


def isStopWord(word):
    stopWords = ['of', 'a', 'is', 'are', 'to', 'he', 'she', 'it', 'an', 'and',
                 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'in', 'its',
                 'on', 'that', 'the', 'was', 'were', 'will', 'with',
                 'so', 'but', 'or', 'his', 'her', 'him', 'them', 'am', 'their',
                 'this', 'us', 'ourselves', 'hers', 'between', 'yourself',
                 'again', 'there', 'about', 'once', 'during', 'out', 'very',
                 'having', 'they', 'own', 'some', 'do', 'yours', 'such', 'into',
                 'most', 'itself', 'other', 'off', 's', 'who', 'each',
                 'themselves', 'until', 'below', 'we', 'these', 'your',
                 'through', 'don', 'nor', 'me', 'more', 'himself', 'down',
                 'should', 'our', 'while', 'above', 'both', 'up', 'ours',
                 'had', 'all', 'no', 'when', 'any', 'before', 'same', 'been',
                 'have', 'does', 'yourselves', 'then', 'because', 'what',
                 'over', 'why', 'can', 'did', 'not', 'now', 'under', 'you',
                 'herself', 'just', 'where', 'too', 'only', 'myself', 'which',
                 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if',
                 'theirs', 'my', 'against', 'doing', 'how', 'further', 'here',
                 'than']
    stopWords2 = ["0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2",
                 "a3", "a4", "ab", "able", "about", "above", "abst", "ac",
                 "accordance", "according", "accordingly", "across", "act",
                 "actually", "ad", "added", "adj", "ae", "af", "affected", 
                 "affecting", "affects", "after", "afterwards", "ag", "again",
                 "against", "ah", "ain", "ain't", "aj", "al", "all", "allow",
                 "allows", "almost", "alone", "along", "already", "also",
                 "although", "always", "am", "among", "amongst", "amoungst",
                 "amount", "an", "and", "announce", "another", "any", "anybody",
                 "anyhow", "anymore", "anyone", "anything", "anyway", "anyways",
                 "anywhere", "ao", "ap", "apart", "apparently", "appear",
                  "appreciate", "appropriate", "approximately", "ar", "are", 
                  "aren", "arent", "aren't", "arise", "around", "as", "a's",
                  "aside", "ask", "asking", "associated", "at", "au", "auth",
                  "av", "available", "aw", "away", "awfully", "ax", "ay", "az",
                  "b", "b1", "b2", "b3", "ba", "back", "bc", "bd", "be", 
                  "became", "because", "become", "becomes", "becoming",
                  "been", "before", "beforehand", "begin", "beginning",
                  "beginnings", "begins", "behind", "being", "believe",
                  "below", "beside", "besides", "best", "better", "between",
                  "beyond", "bi", "bill", "biol", "bj", "bk", "bl", "bn",
                  "both", "bottom", "bp", "br", "brief", "briefly", "bs", "bt",
                  "bu", "but", "bx", "by", "c", "c1", "c2", "c3", "ca", "call", "came", "can", "cannot", "cant", "can't", "cause", "causes", "cc", "cd", "ce", "certain", "certainly", "cf", "cg", "ch", "changes", "ci", "cit", "cj", "cl", "clearly", "cm", "c'mon", "cn", "co", "com", "come", "comes", "con", "concerning", "consequently", "consider", "considering", "contain", "containing", "contains", "corresponding", "could", "couldn", "couldnt", "couldn't", "course", "cp", "cq", "cr", "cry", "cs", "c's", "ct", "cu", "currently", "cv", "cx", "cy", "cz", "d", "d2", "da", "date", "dc", "dd", "de", "definitely", "describe", "described", "despite", "detail", "df", "di", "did", "didn", "didn't", "different", "dj", "dk", "dl", "do", "does", "doesn", "doesn't", "doing", "don", "done", "don't", "down", "downwards", "dp", "dr", "ds", "dt", "du", "due", "during", "dx", "dy", "e", "e2", "e3", "ea", "each", "ec", "ed", "edu", "ee", "ef", "effect", "eg", "ei", "eight", "eighty", "either", "ej", "el", "eleven", "else", "elsewhere", "em", "empty", "en", "end", "ending", "enough", "entirely", "eo", "ep", "eq", "er", "es", "especially", "est", "et", "et-al", "etc", "eu", "ev", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere", "ex", "exactly", "example", "except", "ey", "f", "f2", "fa", "far", "fc", "few", "ff", "fi", "fifteen", "fifth", "fify", "fill", "find", "fire", "first", "five", "fix", "fj", "fl", "fn", "fo", "followed", "following", "follows", "for", "former", "formerly", "forth", "forty", "found", "four", "fr", "from", "front", "fs", "ft", "fu", "full", "further", "furthermore", "fy", "g", "ga", "gave", "ge", "get", "gets", "getting", "gi", "give", "given", "gives", "giving", "gj", "gl", "go", "goes", "going", "gone", "got", "gotten", "gr", "greetings", "gs", "gy", "h", "h2", "h3", "had", "hadn", "hadn't", "happens", "hardly", "has", "hasn", "hasnt", "hasn't", "have", "haven", "haven't", "having", "he", "hed", "he'd", "he'll", "hello", "help", "hence", "her", "here", "hereafter", "hereby", "herein", "heres", "here's", "hereupon", "hers", "herself", "hes", "he's", "hh", "hi", "hid", "him", "himself", "his", "hither", "hj", "ho", "home", "hopefully", "how", "howbeit", "however", "how's", "hr", "hs", "http", "hu", "hundred", "hy", "i", "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ignored", "ih", "ii", "ij", "il", "i'll", "im", "i'm", "immediate", "immediately", "importance", "important", "in", "inasmuch", "inc", "indeed", "index", "indicate", "indicated", "indicates", "information", "inner", "insofar", "instead", "interest", "into", "invention", "inward", "io", "ip", "iq", "ir", "is", "isn", "isn't", "it", "itd", "it'd", "it'll", "its", "it's", "itself", "iv", "i've", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "just", "k", "ke", "keep", "keeps", "kept", "kg", "kj", "km", "know", "known", "knows", "ko", "l", "l2", "la", "largely", "last", "lately", "later", "latter", "latterly", "lb", "lc", "le", "least", "les", "less", "lest", "let", "lets", "let's", "lf", "like", "liked", "likely", "line", "little", "lj", "ll", "ll", "ln", "lo", "look", "looking", "looks", "los", "lr", "ls", "lt", "ltd", "m", "m2", "ma", "made", "mainly", "make", "makes", "many", "may", "maybe", "me", "mean", "means", "meantime", "meanwhile", "merely", "mg", "might", "mightn", "mightn't", "mill", "million", "mine", "miss", "ml", "mn", "mo", "more", "moreover", "most", "mostly", "move", "mr", "mrs", "ms", "mt", "mu", "much", "mug", "must", "mustn", "mustn't", "my", "myself", "n", "n2", "na", "name", "namely", "nay", "nc", "nd", "ne", "near", "nearly", "necessarily", "necessary", "need", "needn", "needn't", "needs", "neither", "never", "nevertheless", "new", "next", "ng", "ni", "nine", "ninety", "nj", "nl", "nn", "no", "nobody", "non", "none", "nonetheless", "noone", "nor", "normally", "nos", "not", "noted", "nothing", "novel", "now", "nowhere", "nr", "ns", "nt", "ny", "o", "oa", "ob", "obtain", "obtained", "obviously", "oc", "od", "of", "off", "often", "og", "oh", "oi", "oj", "ok", "okay", "ol", "old", "om", "omitted", "on", "once", "one", "ones", "only", "onto", "oo", "op", "oq", "or", "ord", "os", "ot", "other", "others", "otherwise", "ou", "ought", "our", "ours", "ourselves", "out", "outside", "over", "overall", "ow", "owing", "own", "ox", "oz", "p", "p1", "p2", "p3", "page", "pagecount", "pages", "par", "part", "particular", "particularly", "pas", "past", "pc", "pd", "pe", "per", "perhaps", "pf", "ph", "pi", "pj", "pk", "pl", "placed", "please", "plus", "pm", "pn", "po", "poorly", "possible", "possibly", "potentially", "pp", "pq", "pr", "predominantly", "present", "presumably", "previously", "primarily", "probably", "promptly", "proud", "provides", "ps", "pt", "pu", "put", "py", "q", "qj", "qu", "que", "quickly", "quite", "qv", "r", "r2", "ra", "ran", "rather", "rc", "rd", "re", "readily", "really", "reasonably", "recent", "recently", "ref", "refs", "regarding", "regardless", "regards", "related", "relatively", "research", "research-articl", "respectively", "resulted", "resulting", "results", "rf", "rh", "ri", "right", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "said", "same", "saw", "say", "saying", "says", "sc", "sd", "se", "sec", "second", "secondly", "section", "see", "seeing", "seem", "seemed", "seeming", "seems", "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "sf", "shall", "shan", "shan't", "she", "shed", "she'd", "she'll", "shes", "she's", "should", "shouldn", "shouldn't", "should've", "show", "showed", "shown", "showns", "shows", "si", "side", "significant", "significantly", "similar", "similarly", "since", "sincere", "six", "sixty", "sj", "sl", "slightly", "sm", "sn", "so", "some", "somebody", "somehow", "someone", "somethan", "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "sp", "specifically", "specified", "specify", "specifying", "sq", "sr", "ss", "st", "still", "stop", "strongly", "sub", "substantially", "successfully", "such", "sufficiently", "suggest", "sup", "sure", "sy", "system", "sz", "t", "t1", "t2", "t3", "take", "taken", "taking", "tb", "tc", "td", "te", "tell", "ten", "tends", "tf", "th", "than", "thank", "thanks", "thanx", "that", "that'll", "thats", "that's", "that've", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "thered", "therefore", "therein", "there'll", "thereof", "therere", "theres", "there's", "thereto", "thereupon", "there've", "these", "they", "theyd", "they'd", "they'll", "theyre", "they're", "they've", "thickv", "thin", "think", "third", "this", "thorough", "thoroughly", "those", "thou", "though", "thoughh", "thousand", "three", "throug", "through", "throughout", "thru", "thus", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "together", "too", "took", "top", "toward", "towards", "tp", "tq", "tr", "tried", "tries", "truly", "try", "trying", "ts", "t's", "tt", "tv", "twelve", "twenty", "twice", "two", "tx", "u", "u201d", "ue", "ui", "uj", "uk", "um", "un", "under", "unfortunately", "unless", "unlike", "unlikely", "until", "unto", "uo", "up", "upon", "ups", "ur", "us", "use", "used", "useful", "usefully", "usefulness", "uses", "using", "usually", "ut", "v", "va", "value", "various", "vd", "ve", "ve", "very", "via", "viz", "vj", "vo", "vol", "vols", "volumtype", "vq", "vs", "vt", "vu", "w", "wa", "want", "wants", "was", "wasn", "wasnt", "wasn't", "way", "we", "wed", "we'd", "welcome", "well", "we'll", "well-b", "went", "were", "we're", "weren", "werent", "weren't", "we've", "what", "whatever", "what'll", "whats", "what's", "when", "whence", "whenever", "when's", "where", "whereafter", "whereas", "whereby", "wherein", "wheres", "where's", "whereupon", "wherever", "whether", "which", "while", "whim", "whither", "who", "whod", "whoever", "whole", "who'll", "whom", "whomever", "whos", "who's", "whose", "why", "why's", "wi", "widely", "will", "willing", "wish", "with", "within", "without", "wo", "won", "wonder", "wont", "won't", "words", "world", "would", "wouldn", "wouldnt", "wouldn't", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx", "y", "y2", "yes", "yet", "yj", "yl", "you", "youd", "you'd", "you'll", "your", "youre", "you're", "yours", "yourself", "yourselves", "you've", "yr", "ys", "yt", "z", "zero", "zi", "zz"]
    if word in stopWords2: return True
    else: return False


def findInvertedIndex(isNumberTokenize=True):
    docCount = 0
    lineArray = []
    allWordArray = []
    wordCounts = {}
    with zipfile.ZipFile('30Columnists.zip') as z:
        for zipinfo in z.infolist():
            if zipinfo.filename.endswith('.txt') and re.search('raw_texts', zipinfo.filename):
                with z.open(zipinfo) as f:
                    textfile = io.TextIOWrapper(f, encoding='cp1254', newline='')
                    docCount += 1
                    file_name = str(zipinfo.filename)
                    file_name = file_name.split("/")
                    file_name = file_name[-1]
                    wordCounts[file_name] = 0
                    for line in textfile:
                        lineArray = line.strip().lower().split()
                        for ind,word in enumerate(lineArray):
                            #clening point marks
                            if len(re.findall("^\xad+", word)) > 0:
                                word = word[1:]
                                lineArray[ind] = word
                            if len(re.findall("\xad+$", word)) > 0:
                                word = word[:-1]
                                lineArray[ind] = word
                            while(len(re.findall("^[',;.:½¿•<>…/\\\\“‘”’\+\{\}\*\"\[\(\-]+", word)) > 0):
                                word = word[1:]
                                lineArray[ind] = word
                            while(len(re.findall("['.,?!:;½¿•<>…/\\\\“‘”’\+\{\}\*\"\)\]]+$", word)) > 0):
                                word = word[:-1]
                                lineArray[ind] = word
                            if checkWord(word) and not isStopWord(word):
                                if isNumberTokenize:
                                    if len(re.findall("[0-9]+", word)) <= 0:
                                        wordArray = [word, file_name]
                                        allWordArray.append(wordArray)
                                else:
                                    wordArray = [word, file_name]
                                    allWordArray.append(wordArray)
                                wordCounts[file_name] += 1
              
    return allWordArray, docCount, wordCounts


startTime = time.time()
invertedIndex, totalDoc, wordCountDict = findInvertedIndex()
finishTime = time.time()
print("Inverted Index Run Time(sn): ",finishTime-startTime)
print("Total Document Number: ", totalDoc)
print("Total Word Numbers Of Documents: ", wordCountDict)

startTime = time.time()
invertedIndex.sort()
finishTime = time.time()
print("Sorting Run Time: ",finishTime-startTime)

def mergeWords(arr):
    counter = 0
    poppedIndexes = []
    posList = []
    allPostingLists = {}
    mergedArr = []
    tfOfWord = {}
    allTf = {}
    comparingWord = arr[0][0]
    comparingDoc = arr[0][1]
    for i, j in enumerate(arr):
        if comparingWord == j[0] and comparingDoc == j[1]:                
            counter += 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        elif comparingWord == j[0] and comparingDoc != j[1]:
            posList.append(comparingDoc)
            comparingDoc = j[1]
            counter = 1
            tfOfWord[comparingDoc] = counter
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
        else:
            mergedArr.append(comparingWord)
            posList.append(comparingDoc)
            allPostingLists[comparingWord] = copy.deepcopy(posList)
            posList = []
            counter = 1
            comparingWord = j[0]
            comparingDoc = j[1]
            tfOfWord = {}
            tfOfWord[comparingDoc] = 1
            allTf[comparingWord] = copy.deepcopy(tfOfWord)
    return mergedArr,allTf,allPostingLists

startTime = time.time()
mergedInvertedIndex, term_frequency_dict, postingList = mergeWords(invertedIndex)
finishTime = time.time()
print("Merging Inverted Index Run Time: ",finishTime-startTime)

def findAVGDL():
    lenghts = list(wordCountDict.values())
    sumOfLengths = 0
    for val in lenghts:
        sumOfLengths += val
    return sumOfLengths/totalDoc


def findIDF(word):
    return math.log10(totalDoc/len(postingList[word]))


def findBM25(relevant_docs, word):
    allBM25Scores = {}
    for docId, val in relevant_docs.items():
        dl = float(wordCountDict[docId])
        avgdl = float(findAVGDL())
        allBM25Scores[docId] = findIDF(word) * ((val * (K1 + 1)) /
                                                (val + (K1 *((1 - B) + (B * 
                                                            (dl / avgdl ))))))
    return allBM25Scores


def listForBM25(words_in_query):
    BM25 = findBM25(term_frequency_dict[words_in_query[0]], words_in_query[0])
    words_in_query.pop(0)
    for word in words_in_query:
        if word in term_frequency_dict:
            bm25 = findBM25(term_frequency_dict[word], word)
            for doc, val in bm25.items():
                if doc not in BM25:
                    BM25[doc] = val 
                else:
                    BM25[doc] += val
    BM25 = dict(sorted(BM25.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return BM25


def findTFIDF(relevantDocs, word):
    allTFIDFScores = {}
    for docId, val in relevantDocs.items():
        allTFIDFScores[docId] = float(val / wordCountDict[docId]) * findIDF(word)
    return allTFIDFScores


def listForTFIDF(wordsInQuery):
    TFIDF = findTFIDF(term_frequency_dict[wordsInQuery[0]], wordsInQuery[0])
    wordsInQuery.pop(0)
    for word in wordsInQuery:
        if word in term_frequency_dict:
            tfidf = findTFIDF(term_frequency_dict[word], word)
            for doc, val in tfidf.items():
                if doc not in TFIDF:
                    TFIDF[doc] = val 
                else:
                    TFIDF[doc] += val
    TFIDF = dict(sorted(TFIDF.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return TFIDF


def findE(TFi, doc, N):
    return (TFi * wordCountDict[doc]) / N


def findDFI(relevant_docs, N):
    allDFIScores = {}
    TFi = 0
    for tf in relevant_docs.values():
        TFi += tf
    for docId, tf in relevant_docs.items():
        """
        dfi = ((tf - findE(TFi, docId, N)) / findE(TFi, docId, N)) + 1
        allDFIScores[docId] = math.log10(dfi) 
        """
        dfi = ((tf - findE(TFi, docId, N)) / math.sqrt(findE(TFi, docId, N))) + 1
        if dfi > 0 :
            allDFIScores[docId] = math.log10(dfi) 
        else:
            allDFIScores[docId] = math.log10(1)
    return allDFIScores


def listForDFI(words_in_query, N):
    DFI = findDFI(term_frequency_dict[words_in_query[0]], N)
    words_in_query.pop(0)
    for word in words_in_query:
        if word in term_frequency_dict:
            dfi = findDFI(term_frequency_dict[word], N)
            for doc, score in dfi.items():
                if doc not in DFI:
                    DFI[doc] = score 
                else:
                    DFI[doc] += score
    DFI = dict(sorted(DFI.items(),
                                 key = lambda kv:(kv[1], kv[0]), reverse=True))
    return DFI


def search(query, test_related=[]):
    if isStopWord(query):
        print("No result")
        return
    textArr = query.lower().split()
    cleanText = []
    for ind,text in enumerate(textArr):
        if not isStopWord(text):
            cleanText.append(text)
    textArr.clear()
    textArr = cleanText
    for ind,word in enumerate(textArr):
        #clening point marks
        if len(re.findall("^\xad+", word)) > 0:
            word = word[1:]
            textArr[ind] = word
        if len(re.findall("\xad+$", word)) > 0:
            word = word[:-1]
            textArr[ind] = word
        while(len(re.findall("^[',;.:½¿•<>…/\\\\“‘”’\+\{\}\*\"\[\(\-]+", word)) > 0):
            word = word[1:]
            textArr[ind] = word
        while(len(re.findall("['.,?!:;½¿•<>…/\\\\“‘”’\+\{\}\*\"\)\]]+$", word)) > 0):
            word = word[:-1]
            textArr[ind] = word
    
    N = 0
    for val in wordCountDict.values():
        N += val
    resultsTFIDF = listForTFIDF(copy.deepcopy(textArr))
    resultsBM25 = listForBM25(copy.deepcopy(textArr))
    resultsDFI = listForDFI(copy.deepcopy(textArr), N)
    tfidfPrec = 0
    bm25Prec = 0
    dfiPrec = 0
    counter = 0
    print("==================TFIDF===================")
    for page, score in resultsTFIDF.items():
        counter += 1
        if page in test_related:
            tfidfPrec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    tfidfPrec = float(tfidfPrec / listPageSize)
    total_tfidf_prec.append(tfidfPrec)
    print(len(resultsTFIDF.keys()), " results are found with precision ", tfidfPrec)

    print("==================BM25====================")
    
    for page, score in resultsBM25.items():
        counter += 1
        if page in test_related:
            bm25Prec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    bm25Prec = float(bm25Prec / listPageSize)
    total_bm25_prec.append(bm25Prec)
    print(len(resultsBM25.keys()), " results are found with precision ", bm25Prec)

    print("==================DFI=====================")
    
    for page, score in resultsDFI.items():
        counter += 1
        if page in test_related:
            dfiPrec += 1
        print(page + " ", score)
        if counter >= listPageSize:
            counter = 0
            break
    dfiPrec = float(dfiPrec / listPageSize)
    total_dfi_prec.append(dfiPrec)
    print(len(resultsDFI.keys()), " results are found with precision ", dfiPrec)


Query1 = "health insurance cost"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query1, ["24.29.txt", "24.32.txt", "25.6.txt", "25.15.txt", "25.38.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query3 = "health insurance changing after president obama"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query3, ["25.5.txt", "25.3.txt", "5.17.txt", "24.13.txt", "25.17.txt",
                "25.12.txt", "25.10.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query10 = "the biggest Financial crisis"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query10, ['20.25.txt', '20.43.txt', '20.21.txt', '20.13.txt', '2.19.txt'])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query7 = "Many people lack health insurance"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query7, ["24.32.txt", "24.31.txt", "24.29.txt", "25.10.txt", "24.32.txt",
                "25.30.txt", "25.17.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query4 = "people don’t want health insurance because of high cost"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query4, ["24.29.txt", "24.32.txt", "25.6.txt", "25.15.txt", "25.38.txt"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query8 = "people think current health care quality is bad"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query8, ["24.19.txt", "24.29.txt", "24.13.txt", "24.14.txt", "25.17"])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query9 = "next elections will change the health care"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query9, ["24.13.txt", "24.14.txt", "", "", ""])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query6 = "how can we avoid global warming"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query6, ["10.37.txt", "30.36.txt", "", "", ""])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query5 = "results of global warming"
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query5, ["30.36.txt",  "", "30.49.txt", "", ""])
finishTime = time.time()
print("search time: ", finishTime - startTime)

Query2 = 'reason of global warming'
startTime = time.time()
print("Page    ---    Relevance Score")
search(Query2, ["30.36.txt", "10.37.txt", "", "", ""])
finishTime = time.time()
print("search time: ", finishTime - startTime)

print("=============MAP VALUES============")
total = 0
for tfidf_i in total_tfidf_prec:
    total += tfidf_i
print("TFIDF: ", total/10)
total = 0
for bm25_i in total_bm25_prec:
    total += bm25_i
print("OKAPI BM25: ", total/10)
total = 0
for dfi_i in total_dfi_prec:
    total += dfi_i
print("DFI: ", total/10)

with open("data.json", "w") as outfile:
    json.dump(termFrequenceDict, outfile)